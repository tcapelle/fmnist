{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8978850f-5fae-4025-b273-e14ade2cc154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0a5019b2d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import composer\n",
    "from composer import Trainer\n",
    "from composer.algorithms import ChannelsLast, CutMix, LabelSmoothing, BlurPool, RandAugment, MixUp\n",
    "from composer.models import mnist_model\n",
    "\n",
    "from composer.loggers import WandBLogger\n",
    "\n",
    "torch.manual_seed(42) # For replicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7714f9fb-dcff-434d-a3d6-1c2d6f2e90e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"fmnist_bench\"\n",
    "ENTITY = \"capecape\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a3ce474-932d-4fb9-ba96-0ea4a66ba1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \".\"\n",
    "\n",
    "bs = 256\n",
    "lr = 1e-3\n",
    "wd = 1e-3\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "112731d6-1b7f-407a-aed2-1da701df447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandBLogger(project=PROJECT, entity=ENTITY, tags=[\"composer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66a9c214-ee52-4ef7-8fca-b9612a4d06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfms = T.Compose([\n",
    "    T.RandomCrop(28, padding=4), \n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "val_tfms = T.Compose([\n",
    "    # T.Resize((32,32)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "tfms = {\"train\": train_tfms, \"valid\":val_tfms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40258dec-6ce4-4821-9cf5-6f9cd1262112",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.FashionMNIST(data_directory, download=True, train=True, transform=tfms[\"train\"])\n",
    "eval_dataset = datasets.FashionMNIST(data_directory, download=True, train=False, transform=tfms[\"valid\"])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=bs, num_workers=8, pin_memory=True)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=bs, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8784e808-32a2-41cb-84f1-6a7d830a462e",
   "metadata": {},
   "source": [
    "## Timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c59eb567-5fec-4592-ae06-eef7f9420ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from composer.models import ComposerClassifier\n",
    "\n",
    "model_name = \"resnet10t\"\n",
    "\n",
    "timm_model = timm.create_model(model_name, pretrained=False, num_classes=10, in_chans=1)\n",
    "model = ComposerClassifier(timm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ad11f44-9ec4-4980-9f7d-e6470dac8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), weight_decay=wd)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=lr, \n",
    "                       steps_per_epoch=len(train_dataloader), \n",
    "                       epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc22b1a6-c6bb-40e7-86b4-c4c9f3d78c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2gg1v044) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>loss/train/total</td><td>█▆▅▄▃▃▃▃▃▂▂▂▂▂▂▁▂▂▂▁▁▂▂▁▂▁▂▁▂▂▂▁▁▂▁▂▁▂▁▁</td></tr><tr><td>metrics/eval/Accuracy</td><td>▁▃▄▅▆▆▆▇▇▇▇▇▇▇██████</td></tr><tr><td>metrics/eval/CrossEntropy</td><td>█▅▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/train/Accuracy</td><td>▁▃▃▅▅▅▅▆▅▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇██▇██▇▇▇▇</td></tr><tr><td>trainer/batch_idx</td><td>▃▅▁▆▂▇▃▅▁▆▂▇▃▇▁▆▂▇▃▇▁▆▂▇▃▇▃▆▂▆▃▇▃▆▂▆▂▇▃█</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>trainer/grad_accum</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>loss/train/total</td><td>0.2643</td></tr><tr><td>metrics/eval/Accuracy</td><td>0.872</td></tr><tr><td>metrics/eval/CrossEntropy</td><td>0.33687</td></tr><tr><td>metrics/train/Accuracy</td><td>0.89583</td></tr><tr><td>trainer/batch_idx</td><td>234</td></tr><tr><td>trainer/global_step</td><td>4700</td></tr><tr><td>trainer/grad_accum</td><td>1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">1671722280-orthodox-hoatzin</strong>: <a href=\"https://wandb.ai/capecape/fmnist_bench/runs/2gg1v044\" target=\"_blank\">https://wandb.ai/capecape/fmnist_bench/runs/2gg1v044</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221222_151801-2gg1v044/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2gg1v044). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/tcapelle/wandb/fmnist/wandb/run-20221222_152742-27n1r54l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/capecape/fmnist_bench/runs/27n1r54l\" target=\"_blank\">1671722862-diligent-salamander</a></strong> to <a href=\"https://wandb.ai/capecape/fmnist_bench\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_epochs = f\"{epochs}ep\" # Train for 3 epochs because we're assuming Colab environment and hardware\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\" # select the device\n",
    "\n",
    "trainer = composer.trainer.Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    "    max_duration=train_epochs,\n",
    "    optimizers=optimizer,\n",
    "    schedulers=scheduler,\n",
    "    device=device,\n",
    "    precision='amp',\n",
    "    loggers=wandb_logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2648522c-ede6-42fd-82c7-d7a8e49b13ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Config:\n",
      "num_gpus_per_node: 1\n",
      "num_nodes: 1\n",
      "rank_zero_seed: 114437740\n",
      "\n",
      "******************************\n",
      "train          Epoch   0:  100%|| 235/235 [00:06<00:00, 37.83ba/s, loss/train/total=1.0973]                                                                                                                          \n",
      "eval           Epoch   0:  100%|| 40/40 [00:00<00:00, 56.59ba/s, metrics/eval/Accuracy=0.6912]                                                                                                                       \n",
      "train          Epoch   1:  100%|| 235/235 [00:06<00:00, 36.79ba/s, loss/train/total=0.7663]                                                                                                                          \n",
      "eval           Epoch   1:  100%|| 40/40 [00:00<00:00, 56.82ba/s, metrics/eval/Accuracy=0.7515]                                                                                                                       \n",
      "train          Epoch   2:  100%|| 235/235 [00:06<00:00, 38.71ba/s, loss/train/total=0.6174]                                                                                                                          \n",
      "eval           Epoch   2:  100%|| 40/40 [00:00<00:00, 57.12ba/s, metrics/eval/Accuracy=0.7743]                                                                                                                       \n",
      "train          Epoch   3:  100%|| 235/235 [00:06<00:00, 36.81ba/s, loss/train/total=0.5564]                                                                                                                          \n",
      "eval           Epoch   3:  100%|| 40/40 [00:00<00:00, 56.81ba/s, metrics/eval/Accuracy=0.7972]                                                                                                                       \n",
      "train          Epoch   4:  100%|| 235/235 [00:06<00:00, 37.22ba/s, loss/train/total=0.4608]                                                                                                                          \n",
      "eval           Epoch   4:  100%|| 40/40 [00:00<00:00, 58.46ba/s, metrics/eval/Accuracy=0.8114]                                                                                                                       \n",
      "train          Epoch   5:  100%|| 235/235 [00:06<00:00, 38.54ba/s, loss/train/total=0.5089]                                                                                                                          \n",
      "eval           Epoch   5:  100%|| 40/40 [00:00<00:00, 58.45ba/s, metrics/eval/Accuracy=0.8226]                                                                                                                       \n",
      "train          Epoch   6:  100%|| 235/235 [00:06<00:00, 37.43ba/s, loss/train/total=0.4535]                                                                                                                          \n",
      "eval           Epoch   6:  100%|| 40/40 [00:00<00:00, 57.05ba/s, metrics/eval/Accuracy=0.8334]                                                                                                                       \n",
      "train          Epoch   7:  100%|| 235/235 [00:06<00:00, 37.61ba/s, loss/train/total=0.4174]                                                                                                                          \n",
      "eval           Epoch   7:  100%|| 40/40 [00:00<00:00, 55.10ba/s, metrics/eval/Accuracy=0.8384]                                                                                                                       \n",
      "train          Epoch   8:  100%|| 235/235 [00:06<00:00, 37.55ba/s, loss/train/total=0.3330]                                                                                                                          \n",
      "eval           Epoch   8:  100%|| 40/40 [00:00<00:00, 57.50ba/s, metrics/eval/Accuracy=0.8444]                                                                                                                       \n",
      "train          Epoch   9:  100%|| 235/235 [00:06<00:00, 37.65ba/s, loss/train/total=0.4038]                                                                                                                          \n",
      "eval           Epoch   9:  100%|| 40/40 [00:00<00:00, 57.46ba/s, metrics/eval/Accuracy=0.8464]                                                                                                                       \n",
      "train          Epoch  10:  100%|| 235/235 [00:06<00:00, 37.64ba/s, loss/train/total=0.3174]                                                                                                                          \n",
      "eval           Epoch  10:  100%|| 40/40 [00:00<00:00, 59.27ba/s, metrics/eval/Accuracy=0.8494]                                                                                                                       \n",
      "train          Epoch  11:  100%|| 235/235 [00:06<00:00, 38.16ba/s, loss/train/total=0.3481]                                                                                                                          \n",
      "eval           Epoch  11:  100%|| 40/40 [00:00<00:00, 59.24ba/s, metrics/eval/Accuracy=0.8540]                                                                                                                       \n",
      "train          Epoch  12:   38%|| 90/235 [00:02<00:03, 40.05ba/s, loss/train/total=0.4273]                                                                                                                           "
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>loss/train/total</td><td>█▆▅▄▄▃▃▂▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▂▂▁▂▁▁▁▁▂▁▁▁</td></tr><tr><td>metrics/eval/Accuracy</td><td>▁▄▅▆▆▇▇▇████</td></tr><tr><td>metrics/eval/CrossEntropy</td><td>█▅▄▃▂▂▂▂▁▁▁▁</td></tr><tr><td>metrics/train/Accuracy</td><td>▁▂▄▅▆▆▆▇▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇█▇▇██▇▇█▇████▇███</td></tr><tr><td>trainer/batch_idx</td><td>▂▄▆█▃▅▇▂▅▇▂▅▇▁▃▆▁▃▆█▃▅█▂▄▇▂▄▆▂▄▆▁▃▅▇▃▅▇▃</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>trainer/grad_accum</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>loss/train/total</td><td>0.42729</td></tr><tr><td>metrics/eval/Accuracy</td><td>0.854</td></tr><tr><td>metrics/eval/CrossEntropy</td><td>0.38717</td></tr><tr><td>metrics/train/Accuracy</td><td>0.84375</td></tr><tr><td>trainer/batch_idx</td><td>90</td></tr><tr><td>trainer/global_step</td><td>2910</td></tr><tr><td>trainer/grad_accum</td><td>1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">1671722862-diligent-salamander</strong>: <a href=\"https://wandb.ai/capecape/fmnist_bench/runs/27n1r54l\" target=\"_blank\">https://wandb.ai/capecape/fmnist_bench/runs/27n1r54l</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221222_152742-27n1r54l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train          Epoch  12:   39%|| 91/235 [00:08<00:53,  2.69ba/s, loss/train/total=0.4273]                                                                                                                           "
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pt/lib/python3.9/site-packages/composer/trainer/trainer.py:1612\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_dataloader, train_dataloader_label, train_subset_num_batches, duration, reset_time, schedulers, scale_schedule_ratio, step_schedulers_every_batch, eval_dataloader, eval_subset_num_batches, eval_interval, grad_accum, precision)\u001b[0m\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;66;03m# update scaler since precision was provided\u001b[39;00m\n\u001b[1;32m   1611\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m ClosureGradScaler() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_closures() \u001b[38;5;28;01melse\u001b[39;00m GradScaler()\n\u001b[0;32m-> 1612\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pt/lib/python3.9/site-packages/composer/trainer/trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1760\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mrun_event(Event\u001b[38;5;241m.\u001b[39mAFTER_DATALOADER)\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mrun_event(Event\u001b[38;5;241m.\u001b[39mBATCH_START)\n\u001b[0;32m-> 1763\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m   1764\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainer/global_step\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1765\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrainer/batch_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_in_epoch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1766\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1768\u001b[0m total_loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch(use_grad_scaling)\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_grad_scaling:\n",
      "File \u001b[0;32m~/mambaforge/envs/pt/lib/python3.9/site-packages/composer/loggers/logger.py:67\u001b[0m, in \u001b[0;36mLogger.log_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m     65\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mtimestamp\u001b[38;5;241m.\u001b[39mbatch\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m destination \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdestinations:\n\u001b[0;32m---> 67\u001b[0m     \u001b[43mdestination\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pt/lib/python3.9/site-packages/composer/loggers/wandb_logger.py:119\u001b[0m, in \u001b[0;36mWandBLogger.log_metrics\u001b[0;34m(self, metrics, step)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# wandb.log alters the metrics dictionary object, so we deepcopy to avoid\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# side effects.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m metrics_copy \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(metrics)\n\u001b[0;32m--> 119\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/pt/lib/python3.9/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
